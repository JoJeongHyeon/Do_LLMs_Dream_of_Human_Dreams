# 🧠 LLM은 인간처럼 언어를 이해하는가?  
## ― 가중치 시각화를 통한 '차다' 의미 분화 분석

## 🧭 프로젝트 개요

본 프로젝트는 대형 언어 모델(LLM)이 인간처럼 언어를 처리하는지에 대한 철학적·인지과학적 질문에서 출발합니다.  
특히, "**차다**"와 같은 동음이의어를 다양한 문맥 속에 넣었을 때, 모델이 이를 **사전적 의미 구분처럼 인식하는지**를 LLM 내부의 **토큰 가중치 시각화**를 통해 분석했습니다.

---

## 🧪 연구 가정과 분석 접근

> 가정:  
> **"LLM이 인간처럼 생각한다면, 언어를 다룰 때 사전적 분류 체계에 따라 의미를 구분할 것이다.  
> 반면, 인간처럼 생각하지 않는다면, 사전적 분류 체계가 아닌 다른 방식으로 언어를 처리할 것이다."**

### 실험 개요
- 실험 대상: `"차다"`라는 한국어 단어 (동음이의어, 다의어)
- 문장 데이터: 모두의 말뭉치 (구어체 + 신문체)에서 '차다'가 포함된 문장 수집
- 분석 대상: 각 문맥에서 LLM의 토큰 임베딩 가중치

---

## 🔍 작업 흐름

### 1. `check_corpus.ipynb`
- **기능**: '모두의 말뭉치'에서 `'차다'`가 포함된 문장을 구어체/신문체로 나누어 수집
- **출력**: 의미 분류가 예상되는 다양한 문장 모음

### 2. `Dive_into_Llama_brain.ipynb`
- **모델**: Meta AI의 `LLaMA`
- **분석 방식**:
  - 문장별 토큰화 및 `'차다'` 토큰의 위치 추출
  - LLM의 해당 토큰 embedding을 수집
  - 전체 문장을 대상으로 UMAP을 활용해 임베딩 벡터 차원 축소
  - Plotly를 활용해 시각적 군집화

---

## 📊 주요 시각화 결과

### `1-동음이의구분.png`
- 의미가 서로 다른 두 유형의 `'차다'` 문장이 **UMAP 공간에서 분리되는 것처럼 보이지만**, 일부 혼합 군집이 관찰됨  
- → 사전적 분류와 완전히 일치하지 않지만, 동음이의 구분은 인간과 비슷하다는 것을 관찰할 수 있다.

### `2-문맥차이1.png` ~ `4-문맥차이3.png`
- 구어체 문장군과 신문체 문장군 간의 벡터 위치 차이도 함께 시각화됨
- → 동음이의의 경우와 달리, 다의의 경우에는 인간의 분류 방식과 LLM의 분류 방식이 일치하지 않는다.

---

## 🧰 사용 기술 스택

| 구분 | 도구 |
|------|------|
| 모델 | LLaMA-3.1-8b (HuggingFace) |
| 시각화 | UMAP, Plotly |
| 데이터 | 모두의 말뭉치 (구어, 신문) |
| 언어 | Python, Jupyter Notebook |
| 주요 라이브러리 | transformers, umap-learn, plotly, pandas, numpy |

---

## 📌 결론 및 통찰

- LLaMA는 `"차다"`의 의미를 문맥에 따라 다르게 처리하지만, 그 방식은 **사전적 분류 체계와 일치하지 않음**
- 따라서 LLM은 "**인간처럼 생각하지 않는다**"는 가설에 정합하는 행동을 보임

---

## 🔮 향후 연구 방향

- BERT, GPT 등 다른 모델과의 비교 실험 진행
- ✅ `'차다'` 외 다른 다의어·동음이의어 실험
